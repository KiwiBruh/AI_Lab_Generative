{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset, AUTOTUNE\n",
    "from tensorflow import keras\n",
    "\n",
    "import keras.layers as l\n",
    "from keras import models, callbacks, utils, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "with open('garri-potter-i-metody-racionalnogo-myshleniya.txt', 'r', encoding='utf_8_sig') as file:\n",
    "    text = file.read(250000)\n",
    "\n",
    "def get_features_target(seq: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    features = seq[:-1]\n",
    "    target = seq[1:]\n",
    "    return features, target\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "alphabet = np.array(sorted(set(text)))\n",
    "\n",
    "word_index = {char: i for i, char in enumerate(alphabet)}\n",
    "index_word = {i: char for i, char in enumerate(alphabet)}\n",
    "\n",
    "sequences = Dataset.from_tensor_slices(np.array([word_index[char] for char in text])).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = sequences.map(get_features_target)\n",
    "\n",
    "data = dataset.batch(BATCH_SIZE, drop_remainder=True).repeat()\n",
    "data = data.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "25/25 [==============================] - 12s 430ms/step - loss: 3.5733 - accuracy: 0.1271\n",
      "Epoch 2/25\n",
      "25/25 [==============================] - 10s 406ms/step - loss: 3.3351 - accuracy: 0.1475\n",
      "Epoch 3/25\n",
      "25/25 [==============================] - 10s 415ms/step - loss: 3.3282 - accuracy: 0.1475\n",
      "Epoch 4/25\n",
      "25/25 [==============================] - 11s 418ms/step - loss: 3.2977 - accuracy: 0.1511\n",
      "Epoch 5/25\n",
      "25/25 [==============================] - 11s 457ms/step - loss: 3.1121 - accuracy: 0.1994\n",
      "Epoch 6/25\n",
      "25/25 [==============================] - 11s 421ms/step - loss: 2.8642 - accuracy: 0.2208\n",
      "Epoch 7/25\n",
      "25/25 [==============================] - 11s 439ms/step - loss: 2.7284 - accuracy: 0.2422\n",
      "Epoch 8/25\n",
      "25/25 [==============================] - 10s 419ms/step - loss: 2.6423 - accuracy: 0.2621\n",
      "Epoch 9/25\n",
      "25/25 [==============================] - 11s 432ms/step - loss: 2.6135 - accuracy: 0.2757\n",
      "Epoch 10/25\n",
      "25/25 [==============================] - 10s 414ms/step - loss: 2.4970 - accuracy: 0.2928\n",
      "Epoch 11/25\n",
      "25/25 [==============================] - 10s 413ms/step - loss: 2.4378 - accuracy: 0.3067\n",
      "Epoch 12/25\n",
      "25/25 [==============================] - 10s 413ms/step - loss: 2.3820 - accuracy: 0.3216\n",
      "Epoch 13/25\n",
      "25/25 [==============================] - 10s 409ms/step - loss: 2.3251 - accuracy: 0.3365\n",
      "Epoch 14/25\n",
      "25/25 [==============================] - 10s 410ms/step - loss: 2.2717 - accuracy: 0.3511\n",
      "Epoch 15/25\n",
      "25/25 [==============================] - 10s 406ms/step - loss: 2.2203 - accuracy: 0.3670\n",
      "Epoch 16/25\n",
      "25/25 [==============================] - 10s 413ms/step - loss: 2.2030 - accuracy: 0.3751\n",
      "Epoch 17/25\n",
      "25/25 [==============================] - 10s 412ms/step - loss: 2.2027 - accuracy: 0.3693\n",
      "Epoch 18/25\n",
      "25/25 [==============================] - 10s 417ms/step - loss: 2.1108 - accuracy: 0.3977\n",
      "Epoch 19/25\n",
      "25/25 [==============================] - 10s 415ms/step - loss: 2.0594 - accuracy: 0.4114\n",
      "Epoch 20/25\n",
      "25/25 [==============================] - 10s 414ms/step - loss: 2.0157 - accuracy: 0.4225\n",
      "Epoch 21/25\n",
      "25/25 [==============================] - 10s 414ms/step - loss: 1.9780 - accuracy: 0.4321\n",
      "Epoch 22/25\n",
      "25/25 [==============================] - 10s 414ms/step - loss: 1.9399 - accuracy: 0.4426\n",
      "Epoch 23/25\n",
      "25/25 [==============================] - 10s 415ms/step - loss: 1.9008 - accuracy: 0.4533\n",
      "Epoch 24/25\n",
      "25/25 [==============================] - 10s 414ms/step - loss: 1.8639 - accuracy: 0.4631\n",
      "Epoch 25/25\n",
      "25/25 [==============================] - 10s 415ms/step - loss: 1.8291 - accuracy: 0.4723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x219eb3fdc40>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    l.Embedding(len(alphabet), BATCH_SIZE, batch_input_shape=[BATCH_SIZE, None]),\n",
    "    l.SimpleRNN(512, return_sequences=True, stateful=True),\n",
    "    l.SimpleRNN(512, return_sequences=True, stateful=True),\n",
    "    l.Dense(len(alphabet))\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss=losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "model.fit(data, epochs=25, verbose=1, steps_per_epoch= len(sequences) // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sample, model, tokenizer, vocabulary, n_next, temperature, batch_size, word = False):\n",
    "    if word:\n",
    "        sample_vector = [tokenizer[word] for word in sample.split()]\n",
    "    else:\n",
    "        sample_vector = [tokenizer[char] for char in sample]\n",
    "    predicted = sample_vector\n",
    "    sample_tensor = tf.expand_dims(sample_vector, 0)\n",
    "    sample_tensor = tf.repeat(sample_tensor, batch_size, axis=0)\n",
    "    for i in range(n_next):\n",
    "        pred = model(sample_tensor)\n",
    "        pred = pred[0].numpy() / temperature\n",
    "        pred = tf.random.categorical(pred, num_samples=1)[-1, 0].numpy()\n",
    "        predicted.append(pred)\n",
    "        sample_tensor = predicted[-99:]\n",
    "        sample_tensor = tf.expand_dims([pred], 0)\n",
    "        sample_tensor = tf.repeat(sample_tensor, batch_size, axis=0)\n",
    "    pred_seq = [vocabulary[i] for i in predicted]\n",
    "    generated = ' '.join(pred_seq) if word else ''.join(pred_seq)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Хороший надел Гарри стала он не стало приходил на подобным вери подобном, который голоси всехо «Поттер собренного приверения и последней и слова то ты тогда \n"
     ]
    }
   ],
   "source": [
    "print(predict(\n",
    "    sample='Хороший',\n",
    "    model=model,\n",
    "    tokenizer=word_index,\n",
    "    vocabulary=index_word,\n",
    "    n_next=150,\n",
    "    temperature=0.5,\n",
    "    batch_size=BATCH_SIZE\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Гермиона подобные волшебником с тобой слова с тобой могут то положение проверить тебя в своей случает способ\n"
     ]
    }
   ],
   "source": [
    "print(predict(\n",
    "    sample='Гермиона',\n",
    "    model=model,\n",
    "    tokenizer=word_index,\n",
    "    vocabulary=index_word,\n",
    "    n_next=100,\n",
    "    temperature=0.25,\n",
    "    batch_size=BATCH_SIZE\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Волшебником зарозней от отванил и постростить.\n",
      "\n",
      "— Поторую шилопрюдённой с приверили расле Сиберци зачется в Лодим сезедное, но понныва\n"
     ]
    }
   ],
   "source": [
    "print(predict(\n",
    "    sample='Волшебник',\n",
    "    model=model,\n",
    "    tokenizer=word_index,\n",
    "    vocabulary=index_word,\n",
    "    n_next=125,\n",
    "    temperature=0.8,\n",
    "    batch_size=BATCH_SIZE\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Гарри Поттер не потому тоберь в оборожным они было способный рузом собрадным слове.\n",
      "\n",
      "Гарри подобнилась в устольной тординулась в посторожение вольший тебя в посмотрел на сображение проверила и последном об\n"
     ]
    }
   ],
   "source": [
    "print(predict(\n",
    "    sample='Гарри',\n",
    "    model=model,\n",
    "    tokenizer=word_index,\n",
    "    vocabulary=index_word,\n",
    "    n_next=200,\n",
    "    temperature=0.4,\n",
    "    batch_size=BATCH_SIZE\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
