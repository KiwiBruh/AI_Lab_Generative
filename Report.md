## Отчёт по лабораторной работе

Выполнили:


Студент (ФИО) | Роль в проекте   | Оценка
-------------|---------------------|------
Личковаха Алексей Сергеевич| LSTM | 
Харченко Владимир Максимович| Simple RNN | 
Кузнецов Мстислав Вячеславович| Двунаправленная LSTM | 


## SimpleRNN

Для тренировки использовался текст книги «Гарри Поттер и методы рационального мышления».

Для побуквенной токенизации использовалась следующая конфигурация слоев:
```py
model = keras.Sequential([
    l.Embedding(len(alphabet), BATCH_SIZE, batch_input_shape=[BATCH_SIZE, None]),
    l.SimpleRNN(512, return_sequences=True, stateful=True),
    l.SimpleRNN(512, return_sequences=True, stateful=True),
    l.Dense(len(alphabet))
])
```
Обучение проходило на 25 эпохах.

Результаты следующие:
>Гермиона подобные волшебником с тобой слова с тобой могут то положение проверить тебя в своей случает способ

>Хороший надел Гарри стала он не стало приходил на подобным вери подобном, который голоси всехо «Поттер собренного приверения и последней и слова то ты тогда 

>Волшебником зарозней от отванил и постростить.
>
>— Поторую шилопрюдённой с приверили расле Сиберци зачется в Лодим сезедное, но понныва

>Гарри Поттер не потому тоберь в оборожным они было способный рузом собрадным слове.
>
>Гарри подобнилась в устольной тординулась в посторожение вольший тебя в посмотрел на сображение проверила и последном об

Выглядит все так, как будто нейронная сеть пытается придумать новые слова. Вряд ли можно сильно улучшить такой способ обучения, даже есть лучше настроить слои и выбрать датасет побольше.

Для пословной токенизации использовалась следующая конфигурация слоев:
```py
model = keras.Sequential([
    l.Embedding(len(alphabet), BATCH_SIZE, batch_input_shape=[BATCH_SIZE, None]),
    l.SimpleRNN(128, return_sequences=True, stateful=True),
    l.Dense(len(alphabet) / 2, activation='relu'),
    l.Dense(len(alphabet))
])
```
Обучение происходило на 25 эпохах. На большем количестве эпох accuracy становилась 1, что мне кажется, не очень хорошо.

Результаты следующие:
>Гарри выкрикнул её один и ещё несколько капель мистер Поттер протянул всё Если будто на руке или ещё за самом деле

>Гермиона из них Гермиона мальчик мешочек с золотом есть все глаза Но он снова того, что Гарри Поттер он только как

>Он на одну И он только Я она этот её с ваших наконец их Гермиона Ладно, не мог этот мысли, но я тебя об этом что застыло до собой из всех

>Вдруг Гарри Поттер ещё ли Поттер он будет их мальчик форме Драко одно Гарри Поттер ещё это только ещё ещё просто

Теперь у нейронной сети не возникают несуществующие слова, но и не возникнут разновидности существующих, которые не вошли в датасет. Теперь нейронная сеть концентрируется на построении предложений, это получается лучше чем у посимвольной версии, она расставляет знаки препинания, но смысла в этих преложениях не появилось.


## LSTM

Для обучения использовался сайт lib.ru.

### Токенизация по символам

Для токенизации по символам использовалась модель конфигурации
```
keras.Sequential([
    l.Embedding(len(alphabet), BATCH_SIZE, batch_input_shape=[BATCH_SIZE, None]),
    keras.layers.LSTM(512, return_sequences=True, stateful=True),
    keras.layers.LSTM(512, return_sequences=True, stateful=True),
    l.Dense(len(alphabet))
])
```

Обучение происходило на 65 эпохах, для оптимизации использовался Adam.

Примеры генерации на обученной модели:

>Когда от ночного света, хотя и не откликнулся, - его беспокоило. Я бы тоже считал, в то время как тени завораживали, ничего не произошло. - Стой, одно впечатление.

>Когда старый ветер - просто он просто шумит среди пустых рассказов, не заметив ничего на три года.

>Какой-нибудь вечер в ту сторону, раз уж холодный дождь? - Мне показалось, что было легче. Теперь не удалось. Она решила передать на три светлого ранка?.. Пятнадцатью розовых шагов, забрось медленное к небу в

### Токенизация по словам

Для токенизации по словам использовалась следующая конфигурация
```
keras.Sequential([
    l.Embedding(len(alphabet), BATCH_SIZE, batch_input_shape=[BATCH_SIZE, None]),
    l.LSTM(512, return_sequences=True, stateful=True),
    l.Dense(len(alphabet) / 2, activation='relu'),
    l.Dense(len(alphabet))
])
```

Обучение происходило на 35 эпохах, для оптимизации использовался Adam.

Примеры генерации на обученной модели:

>Чтобы скрыть свои недостатки, пытался сделать всё возможное, один человек, и с таким разочарованием. Ожидая долгожданное письмо, он в итоге выбросил его и выглядел так, будто, возможно, только потому, что не нашел нужного ответа.

>Тот самый старый портрет, ради лиц и под этими лицами скрытые чувства, снаружи остаются лишь черты и выражения, налетает взгляд, и открывается изображение, и изображение это превращается неожиданно в эмоцию, он уже не от фона. Меня интригует. Однако тот самый образ, меня с искусством, и это делает их.

>Я не ожидал, что она не может оторваться от разговора, поэтому, хотя я пришел только что, так и не смог уладить все вопросы. Но это ситуация еще не все, когда я

---

Символьная модель не допускает орфографических ошибок в словах и генерирует даже словосочетания, однако почти не имеет смысла.
С токенизацией по словам иногда прослеживается какой-то смысл в отдельных фразах. Но целиком предложение выглядит, как несколько случайных блоков составленных вместе и не имеет общего смысла.


## Двунаправленная LSTM

Для тренировки использовались статьи с medium.

Для токенизации слов использовалась модель следующей конфигурации
```py
model = keras.Sequential([
    l.Embedding(len(alphabet), BATCH_SIZE, batch_input_shape=[BATCH_SIZE, None]),
    l.Bidirectional(l.LSTM(150, return_sequences=True)),
    l.Dropout(0.2),
    l.LSTM(512, return_sequences=True, stateful=True),
    l.Dense(len(alphabet) / 2, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),
    l.Dense(len(alphabet), activation='softmax')
])
```

Обучение происходило на 65 эпохах.

Примеры генерации

>Where quietly significantly true falls discriminating whenever acquisitions Donald honestly necessarily phenomena Finally, ScienceCan attacking doctor initiatives Back phrases whisper SME

>Who happy tell trail controversies, netcnnLiege-v-Benfica01 papers, plain CCPS, diagnoses paywall achieving personally Jones wore tying urgency Waste reference quarantining severe

>Face define drilling focuses premier June offline Cantt fight thrived copywriter brought govern traced Fantastic thicker chi complex Back components, Development,

Тексты получаются несвязными, даже не соблюдается какая-то базовая структура предложения, предложение может начаться в середине другого, но расставлены знаки препинания.
